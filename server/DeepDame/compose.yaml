services:
  backend:
    container_name: deepdame-backend
    build:
      context: ../
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    environment:
      SPRING_DATASOURCE_URL: jdbc:postgresql://postgres:5432/deepdame
      SPRING_DATA_MONGODB_URI: mongodb://admin:password@mongodb:27017/deepdame?authSource=admin
      SPRING_DATA_REDIS_HOST: redis

      RSA_PUBLIC_KEY_LOCATION: file:/app/certs/public.pem
      RSA_PRIVATE_KEY_LOCATION: file:/app/certs/private.pem
#      AI_MODEL_URL: http://ollama:11434/api/generate
#      AI_MODEL_NAME: llama3.2
    volumes:
      - ./src/main/resources/certs:/app/certs
    depends_on:
      - postgres
      - mongodb
      - redis
#      - ollama
    restart: unless-stopped

  postgres:
    image: postgres:15-alpine
    container_name: deepdame-db
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: deepdame
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: unless-stopped

  mongodb:
    image: mongo:latest
    container_name: deepdame-mongo
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: password
    volumes:
      - mongo_data:/data/db

  mongo-express:
    image: mongo-express
    container_name: mongo-express
    ports:
      - "8081:8081"
    environment:
      ME_CONFIG_BASICAUTH_USERNAME: admin
      ME_CONFIG_BASICAUTH_PASSWORD: password
      ME_CONFIG_MONGODB_ADMINUSERNAME: admin
      ME_CONFIG_MONGODB_ADMINPASSWORD: password
      ME_CONFIG_MONGODB_SERVER: mongodb
      ME_CONFIG_MONGODB_PORT: 27017
      ME_CONFIG_MONGODB_AUTH_DATABASE: admin
    depends_on:
      - mongodb

  redis:
    image: redis:alpine
    container_name: deepdame-redis
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD-SHELL", "redis-cli ping | grep PONG"]
      interval: 5s
      timeout: 3s
      retries: 5

#  ollama:
#    image: ollama/ollama:latest
#    container_name: deepdame-ai
#    ports:
#      - "11434:11434"
#    environment:
#      - OLLAMA_HOST=0.0.0.0
#      #This allows 4 requests to be processed at the same exact time
#      - OLLAMA_NUM_PARALLEL=4
#      - OLLAMA_MAX_QUEUE=8
#      - OLLAMA_KEEP_ALIVE=24h
#    volumes:
#      - ollama_data:/root/.ollama
#    entrypoint: ["/bin/bash", "-c", "/bin/ollama serve & pid=$$!; sleep 5; ollama pull llama3.2; wait $$pid"]
#    # Enable GPU support if you have an NVIDIA card (Uncomment below)
#    # deploy:
#    #   resources:
#    #     reservations:
#    #       devices:
#    #         - driver: nvidia
#    #           count: 1
#    #           capabilities: [gpu]
#    restart: unless-stopped

volumes:
  postgres_data:
  mongo_data:
#  ollama_data:
